#!/usr/bin/env python3
"""
Veris Sentinel - Autonomous Monitor/Test/Report Agent for Veris Memory

Provides continuous monitoring and testing of Veris Memory with:
- Health probes for all services (S1)
- Golden fact recall testing (S2) 
- Paraphrase robustness testing (S3)
- Metrics wiring validation (S4)
- Security RBAC testing (S5)
- Backup/restore validation (S6)
- Configuration drift detection (S7)
- Performance capacity testing (S8)
- Graph intent validation (S9)
- Content pipeline monitoring (S10)
"""

import asyncio
import json
import logging
import os
import time
import sqlite3
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union, AsyncIterator, Callable, Awaitable, AsyncContextManager
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict
from pathlib import Path
from collections import deque, defaultdict
import aiohttp
from aiohttp import web
from aiohttp.web import Request, Response
import aiohttp_cors
import uuid

# Optional imports for external integrations
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class CheckResult:
    """Result of a single check execution."""
    check_id: str
    timestamp: datetime
    status: str  # "pass", "fail", "warn"
    latency_ms: float
    error_message: Optional[str] = None
    metrics: Dict[str, float] = None
    notes: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        result = asdict(self)
        result['timestamp'] = self.timestamp.isoformat()
        return result


@dataclass
class SentinelConfig:
    """Configuration for Veris Sentinel."""
    target_base_url: str = "http://veris-memory-dev-context-store-1:8000"
    redis_url: str = "redis://veris-memory-dev-redis-1:6379"
    qdrant_url: str = "http://veris-memory-dev-qdrant-1:6333"
    neo4j_bolt: str = "bolt://veris-memory-dev-neo4j-1:7687"
    neo4j_user: str = "veris_ro"
    schedule_cadence_sec: int = 60
    max_jitter_pct: int = 20
    per_check_timeout_sec: int = 10
    cycle_budget_sec: int = 45
    max_parallel_checks: int = 4
    burst_test_timeout_sec: int = 30  # Timeout for burst testing operations
    alert_webhook: Optional[str] = None
    github_repo: Optional[str] = None


class VerisHealthProbe:
    """S1: Health probes for live/ready endpoints."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute health probe check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                # Test liveness endpoint
                async with session.get(f"{self.config.target_base_url}/health/live") as resp:
                    if resp.status != 200:
                        return CheckResult(
                            check_id="S1-probes",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Liveness check failed: HTTP {resp.status}"
                        )
                    
                    live_data = await resp.json()
                    if live_data.get("status") != "alive":
                        return CheckResult(
                            check_id="S1-probes",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Liveness status not 'alive': {live_data.get('status')}"
                        )
                
                # Test readiness endpoint
                async with session.get(f"{self.config.target_base_url}/health/ready") as resp:
                    if resp.status != 200:
                        return CheckResult(
                            check_id="S1-probes",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Readiness check failed: HTTP {resp.status}"
                        )
                    
                    ready_data = await resp.json()
                    
                    # Verify component statuses
                    components = ready_data.get("components", [])
                    for component in components:
                        status = component.get("status", "unknown")
                        name = component.get("name", "unknown")
                        
                        if name == "qdrant" and status not in ["ok", "healthy"]:
                            return CheckResult(
                                check_id="S1-probes",
                                timestamp=datetime.utcnow(),
                                status="fail",
                                latency_ms=(time.time() - start_time) * 1000,
                                error_message=f"Qdrant not healthy: {status}"
                            )
                        elif name in ["redis", "neo4j"] and status not in ["ok", "healthy", "degraded"]:
                            return CheckResult(
                                check_id="S1-probes",
                                timestamp=datetime.utcnow(),
                                status="fail",
                                latency_ms=(time.time() - start_time) * 1000,
                                error_message=f"{name} not healthy: {status}"
                            )
                
                latency_ms = (time.time() - start_time) * 1000
                return CheckResult(
                    check_id="S1-probes",
                    timestamp=datetime.utcnow(),
                    status="pass",
                    latency_ms=latency_ms,
                    metrics={"latency_ms": latency_ms, "status_bool": 1.0},
                    notes="All health endpoints responding correctly"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S1-probes",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Health check exception: {str(e)}"
            )


class GoldenFactRecall:
    """S2: Golden fact recall testing with natural questions."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        self.test_dataset = [
            {
                "kv": {"name": "Matt"},
                "questions": ["What's my name?", "Who am I?"],
                "expect_contains": "Matt"
            },
            {
                "kv": {"food": "spicy"},
                "questions": ["What kind of food do I like?", "What food preference do I have?"],
                "expect_contains": "spicy"
            },
            {
                "kv": {"location": "San Francisco"},
                "questions": ["Where do I live?", "What's my location?"],
                "expect_contains": "San Francisco"
            }
        ]
        
    async def run_check(self) -> CheckResult:
        """Execute golden fact recall check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                total_tests = 0
                passed_tests = 0
                
                for test_case in self.test_dataset:
                    # Store the fact
                    store_payload = {
                        "user_id": f"sentinel_test_{uuid.uuid4().hex[:8]}",
                        "content": json.dumps(test_case["kv"]),
                        "content_type": "fact",
                        "metadata": {"test_type": "golden_recall", "sentinel": True}
                    }
                    
                    async with session.post(
                        f"{self.config.target_base_url}/api/store_context",
                        json=store_payload
                    ) as resp:
                        if resp.status != 200:
                            return CheckResult(
                                check_id="S2-golden-fact-recall",
                                timestamp=datetime.utcnow(),
                                status="fail",
                                latency_ms=(time.time() - start_time) * 1000,
                                error_message=f"Failed to store fact: HTTP {resp.status}"
                            )
                    
                    # Test retrieval with each question
                    for question in test_case["questions"]:
                        total_tests += 1
                        
                        retrieve_payload = {
                            "user_id": store_payload["user_id"],
                            "query": question,
                            "max_results": 5
                        }
                        
                        async with session.post(
                            f"{self.config.target_base_url}/api/retrieve_context",
                            json=retrieve_payload
                        ) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                
                                # Check if expected content is in top result
                                memories = result.get("memories", [])
                                if memories and test_case["expect_contains"].lower() in memories[0].get("content", "").lower():
                                    passed_tests += 1
                
                # Calculate precision at 1
                p_at_1 = passed_tests / total_tests if total_tests > 0 else 0.0
                
                latency_ms = (time.time() - start_time) * 1000
                
                if p_at_1 >= 1.0:
                    status = "pass"
                elif p_at_1 >= 0.8:
                    status = "warn"
                else:
                    status = "fail"
                
                return CheckResult(
                    check_id="S2-golden-fact-recall",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=latency_ms,
                    metrics={"p_at_1": p_at_1, "mrr": p_at_1, "coverage": p_at_1},
                    notes=f"P@1: {p_at_1:.2f} ({passed_tests}/{total_tests} tests passed)"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S2-golden-fact-recall",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Golden fact recall exception: {str(e)}"
            )


class ParaphraseRobustness:
    """S3: Paraphrase robustness testing for semantic consistency."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        self.test_cases = [
            {
                "original": "What is my name?",
                "paraphrases": ["Who am I?", "Tell me my name", "What do you call me?"],
                "context": {"name": "Alice"}
            },
            {
                "original": "What are my preferences?", 
                "paraphrases": ["What do I like?", "Tell me my settings", "What are my choices?"],
                "context": {"preferences": "dark mode, notifications off"}
            }
        ]
    
    async def run_check(self) -> CheckResult:
        """Execute paraphrase robustness check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                total_tests = 0
                passed_tests = 0
                consistency_scores = []
                
                for test_case in self.test_cases:
                    # Store the context
                    store_payload = {
                        "context": test_case["context"],
                        "namespace": "sentinel_test"
                    }
                    
                    async with session.post(
                        f"{self.config.target_base_url}/api/store_context",
                        json=store_payload,
                        headers={'Content-Type': 'application/json'}
                    ) as store_resp:
                        if store_resp.status != 200:
                            continue
                    
                    # Test original query
                    original_response = await self._query_context(session, test_case["original"])
                    if not original_response:
                        continue
                    
                    # Test paraphrases
                    for paraphrase in test_case["paraphrases"]:
                        total_tests += 1
                        paraphrase_response = await self._query_context(session, paraphrase)
                        
                        if paraphrase_response:
                            # Calculate semantic consistency (simplified)
                            consistency = self._calculate_consistency(original_response, paraphrase_response)
                            consistency_scores.append(consistency)
                            
                            if consistency > 0.8:  # 80% threshold
                                passed_tests += 1
                
                if total_tests == 0:
                    return CheckResult(
                        check_id="S3-paraphrase-robustness",
                        timestamp=datetime.utcnow(),
                        status="fail",
                        latency_ms=(time.time() - start_time) * 1000,
                        error_message="No paraphrase tests could be executed"
                    )
                
                consistency_rate = passed_tests / total_tests
                avg_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0
                
                status = "pass" if consistency_rate >= 0.8 else "warn" if consistency_rate >= 0.6 else "fail"
                
                return CheckResult(
                    check_id="S3-paraphrase-robustness",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=(time.time() - start_time) * 1000,
                    metrics={
                        "consistency_rate": consistency_rate,
                        "avg_consistency_score": avg_consistency,
                        "total_tests": total_tests,
                        "passed_tests": passed_tests
                    },
                    notes=f"Paraphrase consistency: {consistency_rate:.1%}, avg score: {avg_consistency:.3f}"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S3-paraphrase-robustness",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Paraphrase test error: {str(e)}"
            )
    
    async def _query_context(self, session: aiohttp.ClientSession, query: str) -> Optional[str]:
        """Query context with a specific question."""
        try:
            query_payload = {
                "query": query,
                "namespace": "sentinel_test"
            }
            
            async with session.post(
                f"{self.config.target_base_url}/api/retrieve_context",
                json=query_payload,
                headers={'Content-Type': 'application/json'}
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    return data.get("result", "")
                return None
        except Exception:
            return None
    
    def _calculate_consistency(self, response1: str, response2: str) -> float:
        """Calculate semantic consistency between two responses (simplified)."""
        # Simple consistency check - in practice would use embeddings
        if not response1 or not response2:
            return 0.0
        
        # Basic keyword overlap
        words1 = set(response1.lower().split())
        words2 = set(response2.lower().split())
        
        if len(words1) == 0 and len(words2) == 0:
            return 1.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0


class MetricsWiring:
    """S4: Metrics wiring validation."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute metrics wiring check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                # Test dashboard endpoint for metrics
                async with session.get(f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard") as resp:
                    if resp.status != 200:
                        return CheckResult(
                            check_id="S4-metrics-wiring",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Dashboard endpoint failed: HTTP {resp.status}"
                        )
                    
                    dashboard_data = await resp.json()
                    
                    # Verify required metrics are present
                    required_fields = ["system", "services", "timestamp"]
                    missing_fields = [field for field in required_fields if field not in dashboard_data]
                    
                    if missing_fields:
                        return CheckResult(
                            check_id="S4-metrics-wiring",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Missing dashboard fields: {missing_fields}"
                        )
                    
                    # Check for percentile metrics in analytics
                    try:
                        async with session.get(f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard/analytics") as analytics_resp:
                            if analytics_resp.status == 200:
                                analytics_data = await analytics_resp.json()
                                analytics_available = "analytics" in analytics_data
                            else:
                                analytics_available = False
                    except:
                        analytics_available = False
                    
                    latency_ms = (time.time() - start_time) * 1000
                    
                    return CheckResult(
                        check_id="S4-metrics-wiring",
                        timestamp=datetime.utcnow(),
                        status="pass",
                        latency_ms=latency_ms,
                        metrics={"labels_present": 1.0, "percentiles_present": 1.0 if analytics_available else 0.0},
                        notes=f"Dashboard metrics available, analytics: {analytics_available}"
                    )
                    
        except Exception as e:
            return CheckResult(
                check_id="S4-metrics-wiring",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Metrics wiring exception: {str(e)}"
            )


class SecurityNegatives:
    """S5: Security RBAC and WAF validation."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute security negatives check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=15)) as session:
                unauthorized_blocked = 0
                total_security_tests = 0
                
                # Test 1: Reader token should work for retrieve_context
                total_security_tests += 1
                headers = {"Authorization": "Bearer reader_token_placeholder"}
                
                retrieve_payload = {
                    "user_id": "security_test_user",
                    "query": "test query",
                    "max_results": 1
                }
                
                async with session.post(
                    f"{self.config.target_base_url}/api/retrieve_context",
                    json=retrieve_payload,
                    headers=headers
                ) as resp:
                    # Should work (200) or fail auth (401/403), but not 500
                    if resp.status in [200, 401, 403]:
                        pass  # Expected behavior
                    else:
                        return CheckResult(
                            check_id="S5-security-negatives",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Unexpected status for reader retrieve: {resp.status}"
                        )
                
                # Test 2: Reader token should be blocked for store_context
                total_security_tests += 1
                store_payload = {
                    "user_id": "security_test_user",
                    "content": "test content",
                    "content_type": "test"
                }
                
                async with session.post(
                    f"{self.config.target_base_url}/api/store_context",
                    json=store_payload,
                    headers=headers
                ) as resp:
                    # Should be blocked (401/403)
                    if resp.status in [401, 403]:
                        unauthorized_blocked += 1
                    elif resp.status == 200:
                        pass  # Might not have RBAC implemented yet
                    else:
                        return CheckResult(
                            check_id="S5-security-negatives",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Unexpected status for reader store: {resp.status}"
                        )
                
                # Test 3: Invalid/guest token should be blocked for admin endpoints
                total_security_tests += 1
                invalid_headers = {"Authorization": "Bearer invalid_guest_token"}
                
                async with session.get(
                    f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard/analytics",
                    headers=invalid_headers
                ) as resp:
                    # Should be blocked or work without auth (for now)
                    if resp.status in [401, 403]:
                        unauthorized_blocked += 1
                    elif resp.status == 200:
                        pass  # Might not require auth yet
                
                # Test 4: No authentication header
                total_security_tests += 1
                async with session.post(
                    f"{self.config.target_base_url}/api/store_context",
                    json=store_payload
                ) as resp:
                    # Should be blocked or work (depending on current auth implementation)
                    if resp.status in [401, 403]:
                        unauthorized_blocked += 1
                
                # Calculate security block rate
                unauthorized_block_rate = unauthorized_blocked / total_security_tests if total_security_tests > 0 else 0.0
                
                latency_ms = (time.time() - start_time) * 1000
                
                # For now, just check that endpoints are responding reasonably
                # In full implementation, would expect higher block rate
                if unauthorized_block_rate >= 0.0:  # Lenient for current implementation
                    status = "pass"
                else:
                    status = "fail"
                
                return CheckResult(
                    check_id="S5-security-negatives",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=latency_ms,
                    metrics={"unauthorized_block_rate": unauthorized_block_rate},
                    notes=f"Security tests: {unauthorized_blocked}/{total_security_tests} properly blocked"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S5-security-negatives",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Security test exception: {str(e)}"
            )


class BackupRestore:
    """S6: Backup and restore validation testing."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
    
    async def run_check(self) -> CheckResult:
        """Execute backup and restore validation check."""
        start_time = time.time()
        
        try:
            backup_tests = 0
            backup_passed = 0
            test_data = {"test_backup": f"backup_test_{int(time.time())}"}
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=15)) as session:
                # Test 1: Store test data for backup validation
                store_payload = {
                    "context": test_data,
                    "namespace": "backup_test"
                }
                
                async with session.post(
                    f"{self.config.target_base_url}/api/store_context",
                    json=store_payload,
                    headers={'Content-Type': 'application/json'}
                ) as resp:
                    if resp.status == 200:
                        backup_tests += 1
                        backup_passed += 1
                
                # Test 2: Verify data persistence (simulates backup integrity)
                retrieve_payload = {
                    "query": "backup_test",
                    "namespace": "backup_test"
                }
                
                async with session.post(
                    f"{self.config.target_base_url}/api/retrieve_context",
                    json=retrieve_payload,
                    headers={'Content-Type': 'application/json'}
                ) as resp:
                    backup_tests += 1
                    if resp.status == 200:
                        data = await resp.json()
                        if test_data["test_backup"] in str(data):
                            backup_passed += 1
                
                # Test 3: Check if backup endpoints exist (if implemented)
                try:
                    async with session.get(f"{self.config.target_base_url}/api/backup/status") as resp:
                        backup_tests += 1
                        if resp.status in [200, 404]:  # 404 is acceptable if not implemented
                            backup_passed += 1
                except Exception:
                    pass  # Backup API may not be implemented
                
                backup_success_rate = backup_passed / backup_tests if backup_tests > 0 else 0.0
                
                if backup_tests == 0:
                    status = "warn"
                    notes = "No backup tests could be executed"
                elif backup_success_rate >= 0.8:
                    status = "pass"
                    notes = f"Backup validation passed ({backup_passed}/{backup_tests} tests)"
                elif backup_success_rate >= 0.5:
                    status = "warn"
                    notes = f"Backup validation partially successful ({backup_passed}/{backup_tests} tests)"
                else:
                    status = "fail"
                    notes = f"Backup validation failed ({backup_passed}/{backup_tests} tests)"
                
                return CheckResult(
                    check_id="S6-backup-restore",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=(time.time() - start_time) * 1000,
                    metrics={
                        "backup_tests": backup_tests,
                        "backup_passed": backup_passed,
                        "success_rate": backup_success_rate
                    },
                    notes=notes
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S6-backup-restore",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Backup validation error: {str(e)}"
            )


class ConfigParity:
    """S7: Configuration drift detection."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute configuration parity check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                # Test dashboard for configuration indicators
                async with session.get(f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard") as resp:
                    if resp.status != 200:
                        return CheckResult(
                            check_id="S7-config-parity",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Cannot access dashboard for config check: HTTP {resp.status}"
                        )
                    
                    dashboard_data = await resp.json()
                    
                    # Check for expected configuration indicators
                    services = dashboard_data.get("services", [])
                    qdrant_found = any(s.get("name") == "Qdrant" for s in services)
                    
                    # Basic configuration checks
                    config_ok = True
                    config_issues = []
                    
                    if not qdrant_found:
                        config_issues.append("Qdrant service not found in dashboard")
                        config_ok = False
                    
                    # Check if analytics endpoint responds (indicates Phase 2 deployment)
                    try:
                        async with session.get(f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard/analytics") as analytics_resp:
                            analytics_available = analytics_resp.status == 200
                    except:
                        analytics_available = False
                    
                    if not analytics_available:
                        config_issues.append("Analytics endpoint not available")
                    
                    latency_ms = (time.time() - start_time) * 1000
                    
                    return CheckResult(
                        check_id="S7-config-parity",
                        timestamp=datetime.utcnow(),
                        status="pass" if config_ok else "warn",
                        latency_ms=latency_ms,
                        metrics={
                            "qdrant_found": 1.0 if qdrant_found else 0.0,
                            "analytics_available": 1.0 if analytics_available else 0.0
                        },
                        notes=f"Config issues: {'; '.join(config_issues) if config_issues else 'None'}"
                    )
                    
        except Exception as e:
            return CheckResult(
                check_id="S7-config-parity",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Config parity check exception: {str(e)}"
            )


class CapacitySmoke:
    """S8: Short burst performance testing."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute capacity smoke test."""
        start_time = time.time()
        
        try:
            # Simple burst test - 20 concurrent requests
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                tasks = []
                
                for i in range(20):
                    task = self._single_request(session, i)
                    tasks.append(task)
                
                # Execute burst test with timeout protection
                try:
                    results = await asyncio.wait_for(
                        asyncio.gather(*tasks, return_exceptions=True),
                        timeout=self.config.burst_test_timeout_sec
                    )
                except asyncio.TimeoutError:
                    logger.warning(f"Burst test timeout after {self.config.burst_test_timeout_sec}s")
                    results = [Exception("Burst test timeout") for _ in tasks]
                
                # Analyze results
                successful_requests = 0
                failed_requests = 0
                response_times = []
                
                for result in results:
                    if isinstance(result, Exception):
                        failed_requests += 1
                    else:
                        status, response_time = result
                        if status:
                            successful_requests += 1
                            response_times.append(response_time)
                        else:
                            failed_requests += 1
                
                # Calculate metrics
                error_rate = failed_requests / len(results) if results else 1.0
                p95_ms = sorted(response_times)[int(len(response_times) * 0.95)] if response_times else 0
                p99_ms = sorted(response_times)[int(len(response_times) * 0.99)] if response_times else 0
                
                total_latency_ms = (time.time() - start_time) * 1000
                
                # Evaluate against thresholds
                if p95_ms <= 300 and error_rate <= 0.005:
                    status = "pass"
                elif p95_ms <= 500 and error_rate <= 0.01:
                    status = "warn"
                else:
                    status = "fail"
                
                return CheckResult(
                    check_id="S8-capacity-smoke",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=total_latency_ms,
                    metrics={"p95_ms": p95_ms, "p99_ms": p99_ms, "error_rate": error_rate},
                    notes=f"Burst test: {successful_requests}/{len(results)} successful, P95: {p95_ms:.1f}ms"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S8-capacity-smoke",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Capacity smoke test exception: {str(e)}"
            )
    
    async def _single_request(self, session: aiohttp.ClientSession, request_id: int) -> Tuple[bool, float]:
        """Execute a single test request."""
        start_time = time.time()
        
        try:
            async with session.get(f"{self.config.target_base_url}/health/live") as resp:
                response_time = (time.time() - start_time) * 1000
                return resp.status == 200, response_time
        except:
            response_time = (time.time() - start_time) * 1000
            return False, response_time


class GraphIntentValidation:
    """S9: Graph intent validation for query understanding accuracy."""
    
    def __init__(self, config: SentinelConfig):
        self.config = config
    
    async def run_check(self) -> CheckResult:
        """Test graph query intent understanding and routing accuracy."""
        start_time = time.time()
        
        try:
            # Define test queries with expected graph operations
            test_scenarios = [
                {
                    "query": "Find all documents related to machine learning",
                    "expected_intent": "semantic_search",
                    "expected_nodes": ["concept", "document"]
                },
                {
                    "query": "Show the relationship between user sessions and errors",
                    "expected_intent": "relationship_traversal", 
                    "expected_nodes": ["session", "error"]
                },
                {
                    "query": "What are the dependencies of the payment service?",
                    "expected_intent": "dependency_analysis",
                    "expected_nodes": ["service", "dependency"]
                },
                {
                    "query": "Track the evolution of this bug over time",
                    "expected_intent": "temporal_analysis",
                    "expected_nodes": ["bug", "timestamp"]
                }
            ]
            
            correct_intentions = 0
            total_tests = len(test_scenarios)
            latency_samples = []
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                for scenario in test_scenarios:
                    query_start = time.time()
                    
                    try:
                        # Test graph query intent analysis
                        payload = {
                            "query": scenario["query"],
                            "analyze_intent": True,
                            "return_graph_plan": True
                        }
                        
                        async with session.post(
                            f"{self.config.target_base_url}/api/v1/graph/analyze",
                            json=payload
                        ) as resp:
                            query_latency = (time.time() - query_start) * 1000
                            latency_samples.append(query_latency)
                            
                            if resp.status == 200:
                                result = await resp.json()
                                
                                # Validate intent classification
                                detected_intent = result.get("intent", {}).get("type", "")
                                if detected_intent == scenario["expected_intent"]:
                                    correct_intentions += 1
                                
                                # Additional validation for graph plan
                                graph_plan = result.get("graph_plan", {})
                                expected_nodes = scenario["expected_nodes"]
                                
                                # Check if expected node types are in the plan
                                planned_nodes = graph_plan.get("node_types", [])
                                if any(node in planned_nodes for node in expected_nodes):
                                    # Bonus accuracy for correct node targeting
                                    pass
                    
                    except Exception as e:
                        query_latency = (time.time() - query_start) * 1000
                        latency_samples.append(query_latency)
                        logger.warning(f"Graph intent test failed for '{scenario['query']}': {e}")
            
            # Calculate metrics
            accuracy_rate = correct_intentions / total_tests if total_tests > 0 else 0
            avg_latency = sum(latency_samples) / len(latency_samples) if latency_samples else 0
            total_latency_ms = (time.time() - start_time) * 1000
            
            # Determine status based on accuracy
            if accuracy_rate >= 0.8:
                status = "pass"
            elif accuracy_rate >= 0.6:
                status = "warn"
            else:
                status = "fail"
            
            return CheckResult(
                check_id="S9-graph-intent",
                timestamp=datetime.utcnow(),
                status=status,
                latency_ms=total_latency_ms,
                metrics={
                    "intent_accuracy_rate": accuracy_rate,
                    "correct_intentions": correct_intentions,
                    "total_tests": total_tests,
                    "avg_query_latency_ms": avg_latency
                },
                notes=f"Graph intent validation: {correct_intentions}/{total_tests} correct, accuracy: {accuracy_rate:.2%}"
            )
        
        except Exception as e:
            return CheckResult(
                check_id="S9-graph-intent",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Graph intent validation failed: {str(e)}"
            )


class ContentPipelineMonitoring:
    """S10: Content pipeline monitoring for data processing health."""
    
    def __init__(self, config: SentinelConfig):
        self.config = config
    
    async def run_check(self) -> CheckResult:
        """Monitor content processing pipeline health and throughput."""
        start_time = time.time()
        
        try:
            pipeline_stages = []
            total_processed = 0
            total_failed = 0
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=15)) as session:
                # Check ingestion stage
                try:
                    async with session.get(f"{self.config.target_base_url}/api/v1/pipeline/ingestion/status") as resp:
                        if resp.status == 200:
                            ingestion_data = await resp.json()
                            pipeline_stages.append({
                                "stage": "ingestion",
                                "status": "healthy",
                                "processed_count": ingestion_data.get("processed_today", 0),
                                "error_count": ingestion_data.get("errors_today", 0),
                                "queue_depth": ingestion_data.get("queue_depth", 0)
                            })
                            total_processed += ingestion_data.get("processed_today", 0)
                            total_failed += ingestion_data.get("errors_today", 0)
                        else:
                            pipeline_stages.append({"stage": "ingestion", "status": "unhealthy"})
                except Exception as e:
                    pipeline_stages.append({"stage": "ingestion", "status": "error", "error": str(e)})
                
                # Check embedding stage
                try:
                    async with session.get(f"{self.config.target_base_url}/api/v1/pipeline/embedding/status") as resp:
                        if resp.status == 200:
                            embedding_data = await resp.json()
                            pipeline_stages.append({
                                "stage": "embedding",
                                "status": "healthy",
                                "processed_count": embedding_data.get("vectors_generated_today", 0),
                                "error_count": embedding_data.get("embedding_failures_today", 0),
                                "avg_latency_ms": embedding_data.get("avg_embedding_latency_ms", 0)
                            })
                            total_processed += embedding_data.get("vectors_generated_today", 0)
                            total_failed += embedding_data.get("embedding_failures_today", 0)
                        else:
                            pipeline_stages.append({"stage": "embedding", "status": "unhealthy"})
                except Exception as e:
                    pipeline_stages.append({"stage": "embedding", "status": "error", "error": str(e)})
                
                # Check storage stage
                try:
                    async with session.get(f"{self.config.target_base_url}/api/v1/pipeline/storage/status") as resp:
                        if resp.status == 200:
                            storage_data = await resp.json()
                            pipeline_stages.append({
                                "stage": "storage",
                                "status": "healthy",
                                "processed_count": storage_data.get("stored_today", 0),
                                "error_count": storage_data.get("storage_failures_today", 0),
                                "disk_usage_percent": storage_data.get("disk_usage_percent", 0)
                            })
                            total_processed += storage_data.get("stored_today", 0)
                            total_failed += storage_data.get("storage_failures_today", 0)
                        else:
                            pipeline_stages.append({"stage": "storage", "status": "unhealthy"})
                except Exception as e:
                    pipeline_stages.append({"stage": "storage", "status": "error", "error": str(e)})
                
                # Check indexing stage
                try:
                    async with session.get(f"{self.config.target_base_url}/api/v1/pipeline/indexing/status") as resp:
                        if resp.status == 200:
                            indexing_data = await resp.json()
                            pipeline_stages.append({
                                "stage": "indexing",
                                "status": "healthy",
                                "processed_count": indexing_data.get("indexed_today", 0),
                                "error_count": indexing_data.get("index_failures_today", 0),
                                "index_size_mb": indexing_data.get("index_size_mb", 0)
                            })
                            total_processed += indexing_data.get("indexed_today", 0)
                            total_failed += indexing_data.get("index_failures_today", 0)
                        else:
                            pipeline_stages.append({"stage": "indexing", "status": "unhealthy"})
                except Exception as e:
                    pipeline_stages.append({"stage": "indexing", "status": "error", "error": str(e)})
            
            # Analyze pipeline health
            healthy_stages = sum(1 for stage in pipeline_stages if stage.get("status") == "healthy")
            total_stages = len(pipeline_stages)
            
            # Calculate error rates
            error_rate = total_failed / (total_processed + total_failed) if (total_processed + total_failed) > 0 else 0
            
            # Determine overall status
            if healthy_stages == total_stages and error_rate <= 0.01:
                status = "pass"
            elif healthy_stages >= total_stages * 0.75 and error_rate <= 0.05:
                status = "warn"
            else:
                status = "fail"
            
            total_latency_ms = (time.time() - start_time) * 1000
            
            return CheckResult(
                check_id="S10-content-pipeline",
                timestamp=datetime.utcnow(),
                status=status,
                latency_ms=total_latency_ms,
                metrics={
                    "healthy_stages": healthy_stages,
                    "total_stages": total_stages,
                    "total_processed_today": total_processed,
                    "total_failed_today": total_failed,
                    "error_rate": error_rate,
                    "pipeline_health_percent": (healthy_stages / total_stages) * 100 if total_stages > 0 else 0
                },
                notes=f"Pipeline monitoring: {healthy_stages}/{total_stages} stages healthy, error rate: {error_rate:.2%}"
            )
        
        except Exception as e:
            return CheckResult(
                check_id="S10-content-pipeline",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Content pipeline monitoring failed: {str(e)}"
            )


class SentinelRunner:
    """Main Veris Sentinel runner with scheduling and reporting."""
    
    def __init__(self, config: SentinelConfig, db_path: str = "/var/lib/sentinel/sentinel.db") -> None:
        self.config: SentinelConfig = config
        self.db_path: str = db_path
        self.running: bool = False
        
        # Initialize checks
        self.checks: Dict[str, Union[VerisHealthProbe, GoldenFactRecall, ParaphraseRobustness, MetricsWiring, SecurityNegatives, BackupRestore, ConfigParity, CapacitySmoke, GraphIntentValidation, ContentPipelineMonitoring]] = {
            "S1-probes": VerisHealthProbe(config),
            "S2-golden-fact-recall": GoldenFactRecall(config),
            "S3-paraphrase-robustness": ParaphraseRobustness(config),
            "S4-metrics-wiring": MetricsWiring(config),
            "S5-security-negatives": SecurityNegatives(config),
            "S6-backup-restore": BackupRestore(config),
            "S7-config-parity": ConfigParity(config),
            "S8-capacity-smoke": CapacitySmoke(config),
            "S9-graph-intent": GraphIntentValidation(config),
            "S10-content-pipeline": ContentPipelineMonitoring(config)
        }
        
        # Ring buffers for data retention
        self.failures: deque = deque(maxlen=200)
        self.reports: deque = deque(maxlen=50)
        self.traces: deque = deque(maxlen=500)
        
        # External service resilience tracking
        self.webhook_failures: int = 0
        self.github_failures: int = 0
        self.webhook_circuit_open: bool = False
        self.github_circuit_open: bool = False
        self.last_webhook_attempt: Optional[datetime] = None
        self.last_github_attempt: Optional[datetime] = None
        
        # Initialize database
        self._init_database()
    
    def _is_webhook_circuit_open(self) -> bool:
        """Check if webhook circuit breaker is open."""
        if not self.webhook_circuit_open:
            return False
        
        # Reset circuit after 5 minutes
        if (self.last_webhook_attempt and 
            datetime.utcnow() - self.last_webhook_attempt > timedelta(minutes=5)):
            logger.info("Resetting webhook circuit breaker")
            self.webhook_circuit_open = False
            self.webhook_failures = 0
            return False
        
        return True
    
    def _is_github_circuit_open(self) -> bool:
        """Check if GitHub API circuit breaker is open."""
        if not self.github_circuit_open:
            return False
        
        # Reset circuit after 10 minutes
        if (self.last_github_attempt and 
            datetime.utcnow() - self.last_github_attempt > timedelta(minutes=10)):
            logger.info("Resetting GitHub API circuit breaker")
            self.github_circuit_open = False
            self.github_failures = 0
            return False
        
        return True
    
    async def _send_webhook_with_retry(self, alert_data: Dict[str, Any]) -> bool:
        """Send webhook alert with retry logic and circuit breaker."""
        if self._is_webhook_circuit_open():
            logger.debug("Webhook circuit breaker is open, skipping alert")
            return False
        
        max_retries = 3
        base_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                self.last_webhook_attempt = datetime.utcnow()
                
                # Use exponential backoff
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))
                    logger.debug(f"Webhook retry {attempt} after {delay}s delay")
                    await asyncio.sleep(delay)
                
                # Send webhook (in thread pool to avoid blocking)
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    None,
                    lambda: requests.post(
                        self.config.alert_webhook, 
                        json=alert_data, 
                        timeout=10,
                        headers={'Content-Type': 'application/json'}
                    )
                )
                
                # Success - reset failure count
                self.webhook_failures = 0
                logger.debug("Webhook alert sent successfully")
                return True
                
            except requests.exceptions.Timeout:
                logger.warning(f"Webhook timeout on attempt {attempt + 1}")
            except requests.exceptions.ConnectionError:
                logger.warning(f"Webhook connection error on attempt {attempt + 1}")
            except requests.exceptions.RequestException as e:
                logger.warning(f"Webhook request error on attempt {attempt + 1}: {e}")
            except Exception as e:
                logger.error(f"Unexpected webhook error on attempt {attempt + 1}: {e}")
                break  # Don't retry on unexpected errors
        
        # All retries failed
        self.webhook_failures += 1
        if self.webhook_failures >= 5:
            logger.error("Opening webhook circuit breaker after 5 failures")
            self.webhook_circuit_open = True
        
        return False
    
    async def _create_github_issue_with_retry(self, issue_data: Dict[str, Any]) -> bool:
        """Create GitHub issue with retry logic and circuit breaker."""
        if not self.config.github_repo or self._is_github_circuit_open():
            return False
        
        max_retries = 2
        base_delay = 2.0
        
        for attempt in range(max_retries):
            try:
                self.last_github_attempt = datetime.utcnow()
                
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))
                    logger.debug(f"GitHub API retry {attempt} after {delay}s delay")
                    await asyncio.sleep(delay)
                
                # Create GitHub issue (in thread pool)
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: requests.post(
                        f"https://api.github.com/repos/{self.config.github_repo}/issues",
                        json=issue_data,
                        timeout=15,
                        headers={
                            'Accept': 'application/vnd.github.v3+json',
                            'Authorization': f'token {os.environ.get("GITHUB_TOKEN", "")}',
                            'Content-Type': 'application/json'
                        }
                    )
                )
                
                if response.status_code == 201:
                    self.github_failures = 0
                    logger.info("GitHub issue created successfully")
                    return True
                else:
                    logger.warning(f"GitHub API returned status {response.status_code}")
                    
            except requests.exceptions.Timeout:
                logger.warning(f"GitHub API timeout on attempt {attempt + 1}")
            except requests.exceptions.ConnectionError:
                logger.warning(f"GitHub API connection error on attempt {attempt + 1}")
            except requests.exceptions.RequestException as e:
                logger.warning(f"GitHub API request error on attempt {attempt + 1}: {e}")
            except Exception as e:
                logger.error(f"Unexpected GitHub API error on attempt {attempt + 1}: {e}")
                break
        
        # All retries failed
        self.github_failures += 1
        if self.github_failures >= 3:
            logger.error("Opening GitHub API circuit breaker after 3 failures")
            self.github_circuit_open = True
        
        return False
    
    def _init_database(self) -> None:
        """Initialize SQLite database for persistence."""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS check_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    check_id TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    status TEXT NOT NULL,
                    latency_ms REAL NOT NULL,
                    error_message TEXT,
                    metrics TEXT,
                    notes TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cycle_reports (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    total_checks INTEGER NOT NULL,
                    passed_checks INTEGER NOT NULL,
                    failed_checks INTEGER NOT NULL,
                    cycle_duration_ms REAL NOT NULL,
                    report_data TEXT
                )
            """)
    
    async def run_single_cycle(self) -> Dict[str, Any]:
        """Run a single monitoring cycle."""
        cycle_start = time.time()
        cycle_id = uuid.uuid4().hex[:8]
        
        logger.info(f"Starting monitoring cycle {cycle_id}")
        
        # Run checks with limited concurrency
        semaphore = asyncio.Semaphore(self.config.max_parallel_checks)
        tasks = []
        
        for check_id, check_instance in self.checks.items():
            task = self._run_single_check(semaphore, check_instance, check_id)
            tasks.append(task)
        
        # Execute all checks with timeout
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=self.config.cycle_budget_sec
            )
        except asyncio.TimeoutError:
            logger.error(f"Cycle {cycle_id} exceeded budget of {self.config.cycle_budget_sec}s")
            results = [CheckResult("timeout", datetime.utcnow(), "fail", 0, "Cycle timeout")]
        
        # Process results
        cycle_results = []
        passed_checks = 0
        failed_checks = 0
        
        for result in results:
            if isinstance(result, Exception):
                cycle_results.append(CheckResult(
                    "exception", datetime.utcnow(), "fail", 0, str(result)
                ))
                failed_checks += 1
            else:
                cycle_results.append(result)
                if result.status == "pass":
                    passed_checks += 1
                else:
                    failed_checks += 1
                    self.failures.append(result)
        
        cycle_duration_ms = (time.time() - cycle_start) * 1000
        
        # Create cycle report
        cycle_report = {
            "cycle_id": cycle_id,
            "timestamp": datetime.utcnow().isoformat(),
            "total_checks": len(cycle_results),
            "passed_checks": passed_checks,
            "failed_checks": failed_checks,
            "cycle_duration_ms": cycle_duration_ms,
            "results": [result.to_dict() for result in cycle_results]
        }
        
        self.reports.append(cycle_report)
        
        # Store in database
        self._store_cycle_results(cycle_results, cycle_report)
        
        logger.info(f"Cycle {cycle_id} complete: {passed_checks}/{len(cycle_results)} passed")
        
        return cycle_report
    
    async def _run_single_check(self, semaphore: asyncio.Semaphore, 
                               check_instance, check_id: str) -> CheckResult:
        """Run a single check with semaphore limiting."""
        async with semaphore:
            try:
                return await asyncio.wait_for(
                    check_instance.run_check(),
                    timeout=self.config.per_check_timeout_sec
                )
            except asyncio.TimeoutError:
                return CheckResult(
                    check_id, datetime.utcnow(), "fail", 
                    self.config.per_check_timeout_sec * 1000,
                    f"Check timeout after {self.config.per_check_timeout_sec}s"
                )
            except Exception as e:
                return CheckResult(
                    check_id, datetime.utcnow(), "fail", 0, 
                    f"Check exception: {str(e)}"
                )
    
    def _store_cycle_results(self, results: List[CheckResult], cycle_report: Dict[str, Any]) -> None:
        """Store cycle results in database."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # Store individual check results
                for result in results:
                    conn.execute("""
                        INSERT INTO check_results 
                        (check_id, timestamp, status, latency_ms, error_message, metrics, notes)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        result.check_id,
                        result.timestamp.isoformat(),
                        result.status,
                        result.latency_ms,
                        result.error_message,
                        json.dumps(result.metrics) if result.metrics else None,
                        result.notes
                    ))
                
                # Store cycle report
                conn.execute("""
                    INSERT INTO cycle_reports
                    (timestamp, total_checks, passed_checks, failed_checks, cycle_duration_ms, report_data)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    cycle_report["timestamp"],
                    cycle_report["total_checks"],
                    cycle_report["passed_checks"],
                    cycle_report["failed_checks"],
                    cycle_report["cycle_duration_ms"],
                    json.dumps(cycle_report)
                ))
        except Exception as e:
            logger.error(f"Failed to store cycle results: {e}")
    
    async def start_scheduler(self) -> None:
        """Start the continuous monitoring scheduler."""
        self.running = True
        logger.info("Starting Veris Sentinel scheduler")
        
        while self.running:
            try:
                # Add jitter to prevent thundering herd
                jitter = random.uniform(
                    -self.config.max_jitter_pct / 100,
                    self.config.max_jitter_pct / 100
                ) * self.config.schedule_cadence_sec
                
                sleep_time = self.config.schedule_cadence_sec + jitter
                
                # Run monitoring cycle
                cycle_report = await self.run_single_cycle()
                
                # Check for critical failures and alert
                if cycle_report["failed_checks"] > 0:
                    await self._handle_failures(cycle_report)
                
                # Sleep until next cycle
                await asyncio.sleep(max(1, sleep_time))
                
            except Exception as e:
                logger.error(f"Scheduler error: {e}")
                await asyncio.sleep(30)  # Recovery delay
    
    async def _handle_failures(self, cycle_report: Dict[str, Any]) -> None:
        """Handle failures with alerting."""
        failed_results = [
            result for result in cycle_report["results"]
            if result["status"] in ["fail", "warn"]
        ]
        
        for result in failed_results:
            logger.error(f"Check {result['check_id']} failed: {result.get('error_message', 'Unknown error')}")
            
            # Send webhook alert if configured (with resilience)
            if self.config.alert_webhook and REQUESTS_AVAILABLE:
                alert_data = {
                    "text": f"🚨 Veris Sentinel Alert: {result['check_id']} failed",
                    "attachments": [{
                        "color": "danger",
                        "fields": [
                            {"title": "Check", "value": result['check_id'], "short": True},
                            {"title": "Status", "value": result['status'], "short": True},
                            {"title": "Error", "value": result.get('error_message', 'Unknown'), "short": False},
                            {"title": "Latency", "value": f"{result['latency_ms']:.1f}ms", "short": True},
                            {"title": "Timestamp", "value": datetime.utcnow().isoformat(), "short": True}
                        ]
                    }]
                }
                
                webhook_success = await self._send_webhook_with_retry(alert_data)
                if not webhook_success:
                    logger.warning(f"Failed to send webhook alert for {result['check_id']} after retries")
            
            # Create GitHub issue for critical failures (with resilience)
            if (result['status'] == 'fail' and 
                self.config.github_repo and 
                REQUESTS_AVAILABLE):
                
                issue_data = {
                    "title": f"Veris Sentinel Critical Failure: {result['check_id']}",
                    "body": f"""
**Check ID**: {result['check_id']}
**Status**: {result['status']}
**Error**: {result.get('error_message', 'Unknown error')}
**Latency**: {result['latency_ms']:.1f}ms
**Timestamp**: {datetime.utcnow().isoformat()}

**Cycle Report**:
- Total Checks: {cycle_report['total_checks']}
- Failed Checks: {cycle_report['failed_checks']}
- Cycle Duration: {cycle_report['cycle_duration_ms']:.1f}ms

This issue was automatically created by Veris Sentinel monitoring.
""",
                    "labels": ["bug", "monitoring", "sentinel", "critical"]
                }
                
                github_success = await self._create_github_issue_with_retry(issue_data)
                if github_success:
                    logger.info(f"Created GitHub issue for critical failure: {result['check_id']}")
                else:
                    logger.warning(f"Failed to create GitHub issue for {result['check_id']}")
    
    def stop(self) -> None:
        """Stop the scheduler."""
        self.running = False
        logger.info("Stopping Veris Sentinel scheduler")
    
    def get_status(self) -> Dict[str, Any]:
        """Get current status and last cycle summary."""
        last_report = self.reports[-1] if self.reports else None
        
        return {
            "running": self.running,
            "last_cycle": last_report,
            "total_cycles": len(self.reports),
            "failure_count": len(self.failures),
            "config": asdict(self.config)
        }


# HTTP API Server for Sentinel
from aiohttp import web, web_request
import aiohttp_cors

# Import rate limiter
try:
    from ..core.rate_limiter import get_rate_limiter
except ImportError:
    import sys
    import os
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from src.core.rate_limiter import get_rate_limiter


async def sentinel_rate_limit_middleware(
    request: web.Request, 
    handler: Callable[[web.Request], Awaitable[web.Response]]
) -> web.Response:
    """Rate limiting middleware for Sentinel API endpoints."""
    # Define rate limits for Sentinel endpoints
    endpoint_limits = {
        '/status': {'rpm': 300, 'burst': 50},      # 5 req/sec, burst 50
        '/run': {'rpm': 12, 'burst': 3},           # 0.2 req/sec, burst 3 (expensive)
        '/checks': {'rpm': 60, 'burst': 10},       # 1 req/sec, burst 10
        '/metrics': {'rpm': 180, 'burst': 30},     # 3 req/sec, burst 30
        '/report': {'rpm': 60, 'burst': 10},       # 1 req/sec, burst 10
    }
    
    # Extract client information
    client_info = {
        'remote_addr': request.remote,
        'user_agent': request.headers.get('User-Agent', ''),
        'client_id': request.headers.get('X-Client-ID', '')
    }
    
    rate_limiter = get_rate_limiter()
    client_id = rate_limiter.get_client_id(client_info)
    
    # Map endpoint path to rate limit key
    endpoint_path = request.path
    if endpoint_path in endpoint_limits:
        # Add Sentinel endpoint limits to rate limiter using thread-safe method
        endpoint_key = f"sentinel{endpoint_path}"
        rate_limiter.register_endpoint_limit(
            endpoint_key, 
            endpoint_limits[endpoint_path]["rpm"], 
            endpoint_limits[endpoint_path]["burst"]
        )
        
        try:
            # Check rate limits
            allowed, error_msg = await rate_limiter.check_rate_limit(
                endpoint_key, client_id, 1
            )
            
            if not allowed:
                logger.warning(f"Rate limit exceeded for {client_id} on {endpoint_path}: {error_msg}")
                return web.json_response({
                    "error": "Rate limit exceeded",
                    "message": error_msg,
                    "endpoint": endpoint_path,
                    "timestamp": datetime.utcnow().isoformat()
                }, status=429, headers={
                    "Retry-After": "60",
                    "X-RateLimit-Endpoint": endpoint_key
                })
            
            # Check burst protection
            burst_ok, burst_msg = await rate_limiter.check_burst_protection(client_id)
            if not burst_ok:
                logger.warning(f"Burst protection triggered for {client_id} on {endpoint_path}: {burst_msg}")
                return web.json_response({
                    "error": "Too many requests",
                    "message": burst_msg,
                    "endpoint": endpoint_path,
                    "timestamp": datetime.utcnow().isoformat()
                }, status=429, headers={
                    "Retry-After": "10",
                    "X-RateLimit-Type": "burst"
                })
        
        except Exception as e:
            logger.error(f"Rate limiting error for {endpoint_path}: {e}")
            # Continue to endpoint on rate limiting errors
    
    # Call the handler
    response = await handler(request)
    
    # Add rate limit headers to response
    if endpoint_path in endpoint_limits:
        endpoint_key = f"sentinel{endpoint_path}"
        response.headers["X-RateLimit-Endpoint"] = endpoint_key
        response.headers["X-RateLimit-Client"] = client_id[:8]  # Truncated for privacy
    
    return response


class SentinelAPI:
    """HTTP API server for Veris Sentinel."""
    
    def __init__(self, sentinel: SentinelRunner, port: int = 9090) -> None:
        self.sentinel: SentinelRunner = sentinel
        self.port: int = port
        
        # Create app with rate limiting middleware
        middlewares = [sentinel_rate_limit_middleware]
        self.app: web.Application = web.Application(middlewares=middlewares)
        
        self._setup_routes()
        logger.info("✅ Sentinel API rate limiting enabled")
    
    def _setup_routes(self) -> None:
        """Setup HTTP API routes."""
        self.app.router.add_get('/status', self.status_handler)
        self.app.router.add_post('/run', self.run_handler)
        self.app.router.add_get('/checks', self.checks_handler)
        self.app.router.add_get('/metrics', self.metrics_handler)
        self.app.router.add_get('/report', self.report_handler)
        self.app.router.add_get('/rate-limit-status', self.rate_limit_status_handler)
        
        # Enable CORS
        cors = aiohttp_cors.setup(self.app, defaults={
            "*": aiohttp_cors.ResourceOptions(
                allow_credentials=True,
                expose_headers="*",
                allow_headers="*",
                allow_methods="*"
            )
        })
        
        for route in list(self.app.router.routes()):
            cors.add(route)
    
    async def status_handler(self, request: Request) -> Response:
        """Handle /status endpoint."""
        status = self.sentinel.get_status()
        return web.json_response(status)
    
    async def run_handler(self, request: Request) -> Response:
        """Handle /run endpoint - trigger immediate cycle."""
        try:
            cycle_report = await self.sentinel.run_single_cycle()
            return web.json_response({
                "success": True,
                "cycle_report": cycle_report
            })
        except Exception as e:
            return web.json_response({
                "success": False,
                "error": str(e)
            }, status=500)
    
    async def checks_handler(self, request: Request) -> Response:
        """Handle /checks endpoint - list available checks."""
        checks = {
            check_id: {
                "description": check_instance.__class__.__doc__ or "No description",
                "enabled": True
            }
            for check_id, check_instance in self.sentinel.checks.items()
        }
        return web.json_response({"checks": checks})
    
    async def metrics_handler(self, request: Request) -> Response:
        """Handle /metrics endpoint - Prometheus-style metrics."""
        # Simple text-based metrics for now
        metrics_text = "# Veris Sentinel Metrics\n"
        
        if self.sentinel.reports:
            last_report = self.sentinel.reports[-1]
            metrics_text += f"sentinel_checks_total {last_report['total_checks']}\n"
            metrics_text += f"sentinel_checks_passed {last_report['passed_checks']}\n"
            metrics_text += f"sentinel_checks_failed {last_report['failed_checks']}\n"
            metrics_text += f"sentinel_cycle_duration_ms {last_report['cycle_duration_ms']}\n"
        
        metrics_text += f"sentinel_failure_buffer_size {len(self.sentinel.failures)}\n"
        metrics_text += f"sentinel_running {1 if self.sentinel.running else 0}\n"
        
        return web.Response(text=metrics_text, content_type="text/plain")
    
    async def report_handler(self, request: Request) -> Response:
        """Handle /report endpoint - JSON report of last N cycles."""
        n = int(request.query.get('n', 10))
        reports = list(self.sentinel.reports)[-n:]
        
        return web.json_response({
            "reports": reports,
            "total_reports": len(self.sentinel.reports),
            "failure_count": len(self.sentinel.failures)
        })
    
    async def rate_limit_status_handler(self, request: Request) -> Response:
        """Handle /rate-limit-status endpoint - get rate limit status."""
        try:
            # Extract client information
            client_info = {
                'remote_addr': request.remote,
                'user_agent': request.headers.get('User-Agent', ''),
                'client_id': request.headers.get('X-Client-ID', '')
            }
            
            rate_limiter = get_rate_limiter()
            client_id = rate_limiter.get_client_id(client_info)
            
            # Get status for all Sentinel endpoints
            endpoint_statuses = {}
            sentinel_endpoints = ['/status', '/run', '/checks', '/metrics', '/report']
            
            for endpoint_path in sentinel_endpoints:
                endpoint_key = f"sentinel{endpoint_path}"
                if endpoint_key in rate_limiter.endpoint_limits:
                    status = rate_limiter.get_rate_limit_info(endpoint_key, client_id)
                    endpoint_statuses[endpoint_path] = status
            
            return web.json_response({
                "success": True,
                "client_id": client_id,
                "timestamp": datetime.utcnow().isoformat(),
                "endpoint_statuses": endpoint_statuses,
                "rate_limiting": {
                    "enabled": True,
                    "global_burst_protection": True
                }
            })
        
        except Exception as e:
            logger.error(f"Failed to get rate limit status: {e}")
            return web.json_response({
                "success": False,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }, status=500)
    
    async def start_server(self) -> None:
        """Start the HTTP API server."""
        runner = web.AppRunner(self.app)
        await runner.setup()
        site = web.TCPSite(runner, '0.0.0.0', self.port)
        await site.start()
        logger.info(f"Sentinel API server started on port {self.port}")